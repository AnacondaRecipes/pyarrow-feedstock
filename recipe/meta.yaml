# Note that this feedstock must be paired with bumping the version of
# `arrow-cpp-feedstock` and the SHA-256 hashes should match between the packages.
{% set version = "21.0.0" %}

package:
  name: pyarrow
  version: {{ version }}

source:
  - url: https://archive.apache.org/dist/arrow/arrow-{{ version }}/apache-arrow-{{ version }}.tar.gz
    sha256: 5d3f8db7e72fb9f65f4785b7a1634522e8d8e9657a445af53d4a34a3849857b5
  # test data (commits taken from submodules)
  # downloading by git is broken on windows workers atm
  {% if not win %}
  - git_url: https://github.com/apache/parquet-testing.git
    git_rev: 18d17540097fca7c40be3d42c167e6bfad90763c
    folder: parquet-testing
  - git_url: https://github.com/apache/arrow-testing.git
    git_rev: fbf6b703dc93d17d75fa3664c5aa2c7873ebaf06
    folder: arrow-testing
  {% endif %}

build:
  number: 0
  skip: true  # [py<39]
  rpaths:
    - lib/
    - {{ SP_DIR }}/pyarrow
  missing_dso_whitelist:
    # These can be found in {{ SP_DIR }}, see the tests below
    - "*/libarrow_python{{ SHLIB_EXT }}"         # [unix]
    - "*/libarrow_python_flight{{ SHLIB_EXT }}"  # [unix]
    - "*/arrow_python.dll"                       # [win]
    - "*/arrow_python_flight.dll"                # [win]
    - "*/arrow_python_parquet_encryption.dll"    # [win]
  ignore_run_exports:
    # Only needed at build time
    - numpy

requirements:
  build:
    - cmake
    - ninja
    - make  # [unix]
    - {{ stdlib('c') }} 
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
  host:
    - arrow-cpp {{ version }}
    - cython >=3
    - numpy 2.0 # [py<313]
    - numpy 2.1 # [py>=313]
    - pip
    - python
    - setuptools >=64
    - setuptools_scm >=8
    - wheel
  run:
    - python

# ValueError: scipy.sparse does not support dtype float16. The only supported types are: bool_, int8, uint8, int16, uint16, int32, uint32, int64, uint64, longlong, ulonglong, float32, float64, longdouble, complex64, complex128, clongdouble.
    # https://github.com/apache/arrow/issues/45229 - temporary disabled:
    #   - pyarrow/tests/test_sparse_tensor.py::test_sparse_coo_tensor_scipy_roundtrip
{% set tests_to_skip = "test_sparse_coo_tensor_scipy_roundtrip" %}

# with pytest.raises(IOError, match="Bucket '.*' not found"):
# Failed: DID NOT RAISE <class 'OSError'>
{% set tests_to_skip = tests_to_skip + " or test_s3_real_aws_region_selection" %}

# pyarrow.lib.ArrowException: Unknown error: Compression block size must be a multiple of memory block size.
{% set tests_to_skip = tests_to_skip + " or test_buffer_readwrite_with_writeoptions" %}

# pyarrow.lib.ArrowNotImplementedError: This Arrow build does not enable mimalloc
# - pyarrow\tests\test_memory.py::test_memory_pool_factories
{% set tests_to_skip = tests_to_skip + " or test_memory_pool_factories" %} # [win]

# Exception: Unrecognized table name
# - pyarrow\tests\test_memory.py::test_supported_memory_backends
{% set tests_to_skip = tests_to_skip + " or test_supported_memory_backends" %} # [win]

# assert False + where False = any(<generator object test_get_library_dirs_win32.<locals>.<genexpr> at 0x0000025041A59E40>)
# - pyarrow\tests\test_misc.py::test_get_library_dirs_win32
{% set tests_to_skip = tests_to_skip + " or test_get_library_dirs_win32" %} # [win]

# Unsupported backend 'mimalloc' specified in ARROW_DEFAULT_MEMORY_POOL (supported backends are 'system')
# - pyarrow\tests\test_memory.py::test_env_var
{% set tests_to_skip = tests_to_skip + " or test_env_var" %} # [win]

test:
  source_files:
    - parquet-testing  # [not win]
    - arrow-testing  # [not win]
  imports:
    - pyarrow
    - pyarrow.lib
    - pyarrow.compute
    - pyarrow.acero
    - pyarrow.csv
    - pyarrow.dataset
    - pyarrow.flight
    - pyarrow.json
    - pyarrow.orc
    - pyarrow.parquet
    - pyarrow.fs
    - pyarrow._s3fs
    - pyarrow._hdfs
  commands:
    - pip check
    # Check for pytests in the site-package dir
    - test -f ${SP_DIR}/pyarrow/tests/test_array.py  # [unix]
    - if not exist %SP_DIR%/pyarrow/tests/test_array.py exit 1  # [win]

    # Check on shared objects
    - test -f ${SP_DIR}/pyarrow/libarrow_python_flight${SHLIB_EXT}  # [unix]
    - test -f ${SP_DIR}/pyarrow/libarrow_python${SHLIB_EXT}  # [unix]
    - if not exist %SP_DIR%\pyarrow\arrow_python.dll exit 1  # [win]
    - if not exist %SP_DIR%\pyarrow\arrow_python_flight.dll exit 1  # [win]

    # Run all the tests:
    # https://arrow.apache.org/docs/developers/guide/step_by_step/testing.html
    - ARROW_TEST_DATA=arrow-testing/data PARQUET_TEST_DATA=parquet-testing/data python -m pytest -v -k "not ({{ tests_to_skip }})" ${SP_DIR}/pyarrow  # [unix]
    - python -m pytest -v -k "not ({{ tests_to_skip }})" %SP_DIR%/pyarrow  # [win]

  requires:
    - pip
    - pytest
    - cffi
    - cloudpickle
    - scipy
    - fsspec
    - hypothesis
    - pandas
    - s3fs >=2023
    - fastparquet

about:
  home: https://github.com/apache/arrow
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE.txt
  summary: Python libraries for Apache Arrow
  description: |
    This library provides a Python API for functionality provided by the Arrow C++ libraries, 
    along with tools for Arrow integration and interoperability with pandas, NumPy, and other 
    software in the Python ecosystem.
  dev_url: https://github.com/apache/arrow/tree/main/python
  doc_url: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst

extra:
  recipe-maintainers:
    - wesm
    - xhochy
    - jreback
    - cpcloud
    - pcmoritz
    - robertnishihara
    - siddharthteotia
    - kou
    - kszucs
    - pitrou 
